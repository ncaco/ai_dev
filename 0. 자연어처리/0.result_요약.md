# 자연어처리(NLP) 핵심 개념 요약

## 자연어처리 기본
- **정의**: 자연어의 이해, 생성, 분석을 다루는 인공지능 연구 분야
- **NLU(자연어 이해)**: 형태/통사/의미 분석을 통해 자연어를 컴퓨터가 이해할 수 있는 형태로 변환 (감정 분석, 개체 독해, 의미론적 유사도 측정)
- **NLG(자연어 생성)**: 컴퓨터가 처리한 결과물을 텍스트/음성으로 변환 (자동 완성, 스토리 생성, 캡션 생성)

## 통계적 언어 모델
- **N-gram**: 연속된 N개 항목을 단위로 처리 (Unigram: 단일 단어, Bigram: 2개 단어, Trigram: 3개 단어)
- **로그 확률**: 작은 확률값 곱셈의 언더플로우 방지, 계산 효율성 향상 (곱셈→덧셈), 확률값 비교 용이
- **라플라스 스무딩**: N-gram의 Zero 확률 문제 해결을 위해 모든 조합에 1을 더해 최소 확률 부여
  - P(wₙ|w₁,...,wₙ₋₁) = (Count(w₁,...,wₙ)+1) / (Count(w₁,...,wₙ₋₁)+V)
- **마르코프 가정**: 현재 상태는 바로 앞 n-1개 상태에만 의존한다는 가정
  - 장점: 계산 단순화, 데이터 희소성 문제 완화
  - 한계: 장기 의존성 무시, 차원의 저주

## 한국어 자연어처리 특징
- **교착어적 특성**: 조사/어미가 풍부하여 어근에 다양한 접사 결합
- **형태소 복잡성**: 불규칙 활용, 음운 변화 다양
- **어휘 희소성 문제**: 형태소 결합으로 생성 가능한 단어 형태가 매우 다양
- **OOV 문제**: 조사/어미의 다양한 결합으로 미등장 단어 빈번

## 임베딩 기법
- **임베딩**: 텍스트를 고정 크기 실수 벡터로 변환, 고차원→저차원 변환으로 의미적 유사성 반영
- **BoW(Bag of Words)**: 단어 순서 무시, 빈도만 고려하는 단순 임베딩 방식
  - 한계: 문맥 정보 미반영, 의미적 유사 단어 구분 불가

### 단어 임베딩
- **Word2Vec**: 단어 임베딩 방법(CBOW, Skip-gram)
  - 네거티브 샘플링: 일부 네거티브 샘플만 사용한 이진 분류로 효율성 향상
- **FastText**: 단어 내부 subword(n-gram) 정보 활용
  - 장점: 오타/희귀 단어에 강함, 형태소 중요 언어에 유리, 학습/추론 빠름

### 문맥 임베딩
- **ELMo**: 문맥에 따라 단어 임베딩이 달라지는 특징
  - 딥 양방향 LSTM으로 깊은 문맥 정보 학습
  - 동음이의어/다의어 구분 가능
- **BERT**: 양방향 트랜스포머 인코더 기반 사전훈련 언어 모델
  - 특징: MLM 방식 양방향 문맥 학습, 사전훈련-미세조정 구조
  - 사전훈련: MLM(마스킹 토큰 예측), NSP(문장 연속성 예측)
  - 응용: 텍스트 분류, 개체명 인식, 질의응답, 문장 쌍 분류

### 문서 임베딩
- **Doc2Vec**: 문서 단위 임베딩 생성 기법
  - Word2Vec 구조 확장, 문서 고유 벡터(태그) 추가 학습
  - DM 모델: 문맥 단어 벡터 + 문서 벡터로 다음 단어 예측
  - DBOW 모델: 문서 벡터만으로 문서 내 임의 단어 예측
  - 활용: 문서 분류, 유사 문서 검색, 문서 군집화 