============================================
결정 트리 - 정보 이득(Information Gain) 계산식
============================================

1. 엔트로피(Entropy) 계산
------------------------
엔트로피는 데이터의 불확실성을 측정하는 지표입니다.

공식: H(S) = -∑[p(i) * log₂(p(i))]
여기서:
- S는 데이터 집합
- p(i)는 클래스 i의 확률(해당 클래스의 샘플 수 / 전체 샘플 수)

2. 정보 이득(Information Gain) 계산
--------------------------------
정보 이득은 특성(속성)에 따라 데이터를 분할했을 때 엔트로피가 얼마나 감소하는지 측정합니다.

공식: IG(S, A) = H(S) - H(S|A)
여기서:
- H(S)는 전체 데이터 집합의 엔트로피
- H(S|A)는 특성 A로 분할한 후의 조건부 엔트로피

3. 조건부 엔트로피(Conditional Entropy) 계산
----------------------------------------
특성 A에 대한 조건부 엔트로피는 다음과 같이 계산됩니다:

공식: H(S|A) = ∑[|S_v|/|S| * H(S_v)]
여기서:
- S_v는 특성 A의 값이 v인 부분 집합
- |S_v|는 부분 집합 S_v의 크기
- |S|는 전체 데이터 집합의 크기
- H(S_v)는 부분 집합 S_v의 엔트로피

4. 동점 처리 규칙
---------------
두 개 이상의 속성이 같은 정보 이득을 가질 경우, 선택 기준은 다음과 같습니다:
- 값의 수가 더 적은 속성을 선호합니다.
- 이는 간단한 모델을 선호하는 오컴의 면도날(Occam's Razor) 원칙과 일치합니다.
- 속성 값이 적을수록 결정 트리가 더 단순해지고 과적합 위험이 줄어듭니다.

예를 들어, 정보 이득이 같을 때:
- 속성 A: 3개의 고유 값 (1, 2, 3)
- 속성 B: 2개의 고유 값 (0, 1)
이 경우 값의 수가 더 적은 속성 B를 선택합니다.

====================================================
예제 데이터에 대한 정보 이득 계산
====================================================

주어진 데이터:
   A  B  C  D Class
0  2  H  1  F     Y
1  1  H  1  F     N
2  1  H  1  T     N
3  3  M  1  F     Y
4  2  M  1  T     Y
5  3  M  1  T     N
6  1  M  1  F     N
7  2  L  0  T     Y
8  1  L  0  F     Y
9  3  L  0  T     N

1. 전체 데이터 엔트로피 계산
---------------------------
- 클래스 Y: 5개 (전체의 50%)
- 클래스 N: 5개 (전체의 50%)

H(S) = -0.5 * log₂(0.5) - 0.5 * log₂(0.5)
     = -0.5 * (-1) - 0.5 * (-1)
     = 0.5 + 0.5
     = 1.0

2. 속성 A에 대한 정보 이득 계산
-----------------------------
A=1인 부분집합 (4개):
- 클래스 Y: 1개 (25%)
- 클래스 N: 3개 (75%)
H(S_A=1) = -0.25 * log₂(0.25) - 0.75 * log₂(0.75)
         = -0.25 * (-2) - 0.75 * (-0.415)
         = 0.5 + 0.311
         = 0.811

A=2인 부분집합 (3개):
- 클래스 Y: 3개 (100%)
- 클래스 N: 0개 (0%)
H(S_A=2) = -1 * log₂(1) - 0 * log₂(0)
         = -1 * 0 - 0
         = 0

A=3인 부분집합 (3개):
- 클래스 Y: 1개 (33.3%)
- 클래스 N: 2개 (66.7%)
H(S_A=3) = -0.333 * log₂(0.333) - 0.667 * log₂(0.667)
         = -0.333 * (-1.585) - 0.667 * (-0.585)
         = 0.528 + 0.390
         = 0.918

조건부 엔트로피 H(S|A):
H(S|A) = (4/10) * 0.811 + (3/10) * 0 + (3/10) * 0.918
       = 0.324 + 0 + 0.275
       = 0.599

정보 이득 IG(S, A):
IG(S, A) = H(S) - H(S|A)
         = 1.0 - 0.599
         = 0.401

3. 속성 B에 대한 정보 이득 계산
-----------------------------
B=H인 부분집합 (3개):
- 클래스 Y: 1개 (33.3%)
- 클래스 N: 2개 (66.7%)
H(S_B=H) = -0.333 * log₂(0.333) - 0.667 * log₂(0.667)
         = 0.918

B=M인 부분집합 (4개):
- 클래스 Y: 2개 (50%)
- 클래스 N: 2개 (50%)
H(S_B=M) = -0.5 * log₂(0.5) - 0.5 * log₂(0.5)
         = 1.0

B=L인 부분집합 (3개):
- 클래스 Y: 2개 (66.7%)
- 클래스 N: 1개 (33.3%)
H(S_B=L) = -0.667 * log₂(0.667) - 0.333 * log₂(0.333)
         = 0.918

조건부 엔트로피 H(S|B):
H(S|B) = (3/10) * 0.918 + (4/10) * 1.0 + (3/10) * 0.918
       = 0.275 + 0.4 + 0.275
       = 0.95

정보 이득 IG(S, B):
IG(S, B) = H(S) - H(S|B)
         = 1.0 - 0.95
         = 0.05

4. 속성 C에 대한 정보 이득 계산
-----------------------------
C=0인 부분집합 (3개):
- 클래스 Y: 2개 (66.7%)
- 클래스 N: 1개 (33.3%)
H(S_C=0) = -0.667 * log₂(0.667) - 0.333 * log₂(0.333)
         = 0.918

C=1인 부분집합 (7개):
- 클래스 Y: 3개 (42.9%)
- 클래스 N: 4개 (57.1%)
H(S_C=1) = -0.429 * log₂(0.429) - 0.571 * log₂(0.571)
         = 0.988

조건부 엔트로피 H(S|C):
H(S|C) = (3/10) * 0.918 + (7/10) * 0.988
       = 0.275 + 0.692
       = 0.967

정보 이득 IG(S, C):
IG(S, C) = H(S) - H(S|C)
         = 1.0 - 0.967
         = 0.033

5. 속성 D에 대한 정보 이득 계산
-----------------------------
D=F인 부분집합 (5개):
- 클래스 Y: 3개 (60%)
- 클래스 N: 2개 (40%)
H(S_D=F) = -0.6 * log₂(0.6) - 0.4 * log₂(0.4)
         = 0.971

D=T인 부분집합 (5개):
- 클래스 Y: 2개 (40%)
- 클래스 N: 3개 (60%)
H(S_D=T) = -0.4 * log₂(0.4) - 0.6 * log₂(0.6)
         = 0.971

조건부 엔트로피 H(S|D):
H(S|D) = (5/10) * 0.971 + (5/10) * 0.971
       = 0.4855 + 0.4855
       = 0.971

정보 이득 IG(S, D):
IG(S, D) = H(S) - H(S|D)
         = 1.0 - 0.971
         = 0.029

====================================================
결론: 각 속성의 정보 이득과 값의 수
====================================================

| 속성 | 정보 이득 | 고유 값의 수 | 고유 값     |
|------|-----------|--------------|------------|
| A    | 0.401     | 3            | 1, 2, 3    |
| B    | 0.05      | 3            | H, M, L    |
| C    | 0.033     | 2            | 0, 1       |
| D    | 0.029     | 2            | F, T       |

속성 A가 가장 높은 정보 이득(0.401)을 가지므로 최상의 분할 기준입니다.

만약 정보 이득이 같은 속성이 있었다면, 값의 수가 더 적은 속성을 선택해야 합니다. 예를 들어, C와 D의 정보 이득이 같았다면(실제로는 약간 차이가 있음), 둘 다 2개의 고유 값을 가지므로 추가적인 기준이 필요할 수 있습니다.

따라서, 정보 이득을 기준으로 선택한다면 속성 A가 가장 좋은 분할 기준이 됩니다.
이것이 결정 트리의 루트 노드로 A를 선택하는 이유입니다. 