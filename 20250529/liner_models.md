
f(x) = ax + b
ŷ = w · X

### Bias-Variance 분해 해설

**Bias-Variance 분해 공식:**
$$
E\left[(f(x) - \hat{f}(x))^2\right] = (f(x) - E[\hat{f}(x)])^2 + E\left[(\hat{f}(x) - E[\hat{f}(x)])^2\right]
$$

- 첫 번째 항 $(f(x) - E[\hat{f}(x)])^2$는 **Bias(편향)**의 제곱입니다.
- 두 번째 항 $E\left[(\hat{f}(x) - E[\hat{f}(x)])^2\right]$는 **Variance(분산)**입니다.

#### Bias (편향)
- 예측 함수의 평균이 실제 함수 $f(x)$와 얼마나 차이가 나는지를 나타냅니다.
- 만약 여러 번 학습을 반복했을 때 예측값의 평균이 실제값과 일치하면 bias는 0입니다.
- 그렇지 않으면 학습 알고리즘에 체계적인 오류(편향)가 존재합니다.

#### Variance (분산)
- 학습 데이터의 변화에 따라 예측 함수가 얼마나 변동하는지를 나타냅니다.
- 데이터셋이 달라질 때마다 예측 결과가 크게 달라진다면 variance가 큽니다.

#### 요약
- **Bias**: 모델이 평균적으로 얼마나 정답에서 벗어나는지(체계적 오류)
- **Variance**: 데이터셋이 바뀔 때 예측이 얼마나 달라지는지(민감도)

---
### 선형모델은
lb, hv = 이후. 2
hb, lv = 우선. 1
1. 디시전 트리모델로 중요.
---
### 선형회귀

(x, y) 가 주어져있고, a, b 를 찾는 문제

h(가설) = w1x1 + w0x0 = ŷ
 = w0x0 + w1x1
 = (w0,w1) * (x0, x1)
 = w0,w1    x0
            x1
 =   w0 ^ t    x0 
     w1      * x1

 = ŷ = w^tX
= err = Y_i - t_i^
---

### 경사하강법(Gradient Descent)이란?

경사하강법은 최적의 직선을 찾기 위해 손실 함수(loss function, 예: MSE)가 최소가 되는 파라미터(가중치 w, 절편 b 등)를 반복적으로 업데이트하는 최적화 알고리즘입니다.

#### 원리 및 과정
- 임의의 파라미터(w)에서 시작합니다.
- 손실 함수의 각 파라미터에 대한 기울기(미분값, $\frac{\partial}{\partial w_i} Loss(\mathbf{w})$)를 계산합니다.
- 각 파라미터를 기울기의 반대 방향(손실이 줄어드는 방향)으로 $\alpha$(학습률, step size)만큼 이동시킵니다.
- 이 과정을 손실 함수가 충분히 작아질 때까지 반복합니다.

#### 수식
$$
w_i \leftarrow w_i - \alpha \frac{\partial}{\partial w_i} Loss(\mathbf{w})
$$

#### 해석
- **경사하강법**은 닫힌 해(closed form solution)가 존재하지 않거나 계산이 어려운 경우에도, 반복적으로 손실을 줄여가며 최적의 파라미터를 찾을 수 있게 해줍니다.
- 학습률 $\alpha$는 한 번에 이동하는 크기를 결정하며, 너무 크면 발산하고, 너무 작으면 수렴이 느려질 수 있습니다.
- $\alpha$는 고정값일 수도 있고, 학습이 진행됨에 따라 점차 줄어들게(Decay) 설정할 수도 있습니다.

#### 요약
- 경사하강법은 선형모델뿐 아니라 복잡한 모델(딥러닝 등)에서도 널리 사용되는 최적화 기법입니다.
- 반복적으로 손실 함수의 기울기를 따라 파라미터를 조정하여, 최적의 직선(또는 결정 경계 등)을 찾아갑니다.
---
### Sigmoid
시그모이드(Sigmoid) 함수는 입력값을 0과 1 사이의 값으로 변환해주는 S자 형태의 비선형 함수입니다. 주로 이진 분류 문제에서 확률(예: "이 샘플이 클래스 1일 확률")을 출력할 때 사용됩니다.

#### 수식
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

#### 특징 및 해석
- 입력 $x$가 매우 크면 출력은 1에 가까워지고, 매우 작으면 0에 가까워집니다.
- 출력값이 항상 (0, 1) 사이이므로, 확률로 해석하기에 적합합니다.
- 미분이 간단하여 역전파(Backpropagation) 계산에 유리합니다.
- 하지만 입력이 너무 크거나 작으면, 기울기가 0에 가까워져 "기울기 소실(Vanishing Gradient)" 문제가 발생할 수 있습니다.

#### 시각적 예시
- $x=0$일 때 $\sigma(0)=0.5$ (중간값)
- $x \gg 0$이면 $\sigma(x) \approx 1$
- $x \ll 0$이면 $\sigma(x) \approx 0$
---

### 우도함수(Likelihood)
- 확률을 최대화 해라.
- 참일 확률, 거짓일 확률.
- 레이블 넣어서 컨트롤함.
- 확률이 가장 높아지는 쪽으로 이동함.
- 결과에 - 붙이면 교차 엔트로피 
- BCE, Binary Cross Entropic

### 멀티 클래스로 하려면
 - 멀티 리스폰스 : 해당 클래스가 아닌것을 구분하는 선을 그림
 - soft max (reqression) : a > b * 1/(a+b) * a, b
 - hard max (reqression) : a > b * 1, 0

### 서포트 벡터 머신 Maximum Margin Separator
 - 힌지로스 
